{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS and IV via GMM\n",
    "#### Author: Jaepil Lee\n",
    "#### Date: 08/26/2020\n",
    "\n",
    "The goal of this note is to see that OLS and IV are special cases of GMM.\n",
    "\n",
    "1. GMM: Concepts and Algorithm\n",
    "2. OLS via GMM\n",
    "3. IV via GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\jaepi\\OneDrive\\Documents\\GitHub\\jaepillee0315.github.io`\n"
     ]
    }
   ],
   "source": [
    "ENV[\"COLUMNS\"]=100; ENV[\"LINES\"]=1000\n",
    "# Importing packages...\n",
    "using Pkg; Pkg.activate(\"./..\"); Pkg.instantiate();\n",
    "using Distributions, Statistics, Random, HypothesisTests,\n",
    "      LinearAlgebra, Distances, SharedArrays, \n",
    "      Optim, ForwardDiff, NLSolversBase, \n",
    "      Plots, Printf, LaTeXStrings, PlotThemes,\n",
    "      CSV, DataFrames, TimeSeries, Dates,\n",
    "      FredApi, FredData;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [How GMM works: OLS and LIV](#ols_liv_GMM)<a name=\"ols_liv_GMM_menu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how GMM estimation routine works in OLS and LIV. Let's construct a hypothetical dataset with $N$ where:\n",
    "- $y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i, \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ for $i=1,\\ldots,N$\n",
    "- $(\\beta_0, \\beta_1, \\beta_2) = (0.15, 0.25, -1.5), \\sigma = 1.2$\n",
    "\n",
    "We all know what $\\beta_{ols}$ looks like: $(X'X)^{-1}X'Y$. We also know its standard errors. Let's code it just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DGP\n",
    "function DGP_OLS(N, β_DGP, σ_DGP)\n",
    "    Random.seed!(42) # RNG: We get the same number for replicability\n",
    "    (β₀,β₁,β₂) = β_DGP\n",
    "    x₁ = randn(N) # a Nx1 vector\n",
    "    x₂ = randn(N) # a Nx1 vector\n",
    "    ε = σ_DGP * randn(N)  # a Nx1 vector\n",
    "    y = β₀ .+ β₁ .* x₁ .+ β₂ .* x₂ .+ ε # a Nx1 vector\n",
    "    X = [ones(N, 1) x₁ x₂] # a Nx3 matrix\n",
    "    # If you wish to use other statistical packages, use this CSV file.\n",
    "    # df = DataFrame(X0 = X[:, 1], X1 = X[:, 2], X2 = X[:, 3], y = y);\n",
    "    # CSV.write(\"./OLS.csv\",df);\n",
    "    return y, X\n",
    "end\n",
    "#%% OLS estimation\n",
    "function OLS_estimation(y, X)\n",
    "    N = length(y)\n",
    "    β_OLS = inv(X'X) * X'y # a 3x1 vector given DGP\n",
    "    ε_OLS = y - X * β_OLS\n",
    "    invXX = inv(X'*X)\n",
    "    Xε = X.*ε_OLS\n",
    "    Σ_OLS = invXX*(Xε'*Xε)*invXX # V_HC0 - White-Heteroskedasticity-Robust VCov\n",
    "    σ_OLS = sqrt.(diag(Σ_OLS))\n",
    "    return β_OLS, σ_OLS\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take $\\mathbf{Y}$ and $\\mathbf{X}$ we generated above and find $\\beta$ via GMM. You will see that they are exactly equal to each other. Let $f_i(\\beta):=f(Y_i,X_i,\\beta)$ be the function of orthogonality conditions evaluated at $i$'s covariates. The orthogonality condition for OLS is $\\mathbb{E}(\\varepsilon_i X_i) = 0$, where $\\varepsilon_i = Y_i - X_i'\\beta$ and the expectation is taken over $i$. In the GMM framework, we wish to find $\\beta$ that satisfies:\n",
    "$$\n",
    "    0 = \\frac{1}{N}\\sum_{i=1}^{N}f_i(\\beta) = \\frac{1}{N}\\sum_{i=1}^{N} \\varepsilon_i X_i= \\frac{1}{N}\\sum_{i=1}^{N} (Y_i - X_i'\\beta) \\times X_i.\n",
    "$$\n",
    "Notice there are $3 \\times 1$ orthogonality conditions: $\\varepsilon_i$ is a scalar and $X_i$ is a $3\\times1$ vector. Thus, the above equation is an \"element-wise\" average of a vector-valued function.\n",
    "\n",
    "If you recall from econometrics I, what OLS really does is minimizing the sum of squared residuals, $\\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i^2$ where $\\varepsilon_i=Y_i-X_i'\\beta$. The first order condition of the minimization problem is found by taking a gradient of the sum of squared residuals (as a function of $\\beta$, $X$, and $Y$) with respect to $\\beta$. \n",
    "$$\n",
    "    \\frac{\\partial}{\\partial\\beta'}\\left(\\frac{1}{N}\\sum_{i}^{N}Y_i^2 - \\frac{1}{N}\\sum_{i}^{N} 2 X_i Y_i \\beta + \\frac{1}{N}\\sum_{i}^{N} \\beta' X_i X_i' \\beta \\right) = \\frac{1}{N}\\sum_{i}^{N}2(X_iX_i'\\beta - X_i Y_i) = 0 \\\\\n",
    "    \\frac{1}{N}\\sum_{i}^{N}(Y_i - X_i'\\beta) \\times X_i = 0\n",
    "$$\n",
    "This is exactly equal to the moment that GMM uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS via GMM\n",
    "function GMM_OLS(y,X)\n",
    "\n",
    "    #=\n",
    "    f_OLS(β,y,X): orthogonality conditions\n",
    "    W: weighting matrix. In OLS, this is the identity matrix.\n",
    "    f_sum(β): a (k x 1) vector of sample average of normal function.\n",
    "    obj_OLS(β): a \"sandwich\" of f_sum(β).\n",
    "    Q: In OLS, it is inv(X'*X).\n",
    "    Ω: In OLS, it is the outer product of (X'*ε).\n",
    "    =#\n",
    "\n",
    "    function f_OLS(β,y,X)\n",
    "        return (y - X*β) .* X\n",
    "    end\n",
    "    \n",
    "    k = size(X)[2]\n",
    "    N = length(y)\n",
    "    W = I;\n",
    "    f_sum(β) = vec(sum(f_OLS(β, y, X),dims=1));\n",
    "    obj_OLS(β) = N * (f_sum(β)/N)' * W * (f_sum(β)/N);\n",
    "    opt = optimize(obj_OLS, randn(k), BFGS(), autodiff = :forward);\n",
    "\n",
    "    β_OLS_GMM = opt.minimizer;\n",
    "    Q = ForwardDiff.jacobian(β -> f_sum(β) , β_OLS_GMM)/N;\n",
    "    Ω = f_OLS(β_OLS_GMM, y, X)'*f_OLS(β_OLS_GMM, y, X)/N;\n",
    "    Σ_OLS_GMM = inv(Q'*W*Q)*(Q'*W*Ω*W*Q)*inv(Q'*W*Q)/N; # remember to divide by N to compute s.e. of β_hat!\n",
    "    σ_OLS_GMM = sqrt.(diag(Σ_OLS_GMM));\n",
    "    \n",
    "    return β_OLS_GMM, σ_OLS_GMM\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the formula for OLS:\n",
      "    |     β₀     |     β₁     |     β₂     | \n",
      "DGP |  0.1500000 |  0.2500000 | -1.5000000 |\n",
      "OLS |  0.1574148 |  0.2556172 | -1.5032818 |\n",
      "    |( 0.0038011)|( 0.0037832)|( 0.0038027)|\n",
      "GMM |  0.1574148 |  0.2556172 | -1.5032818 |\n",
      "    |( 0.0038011)|( 0.0037832)|( 0.0038027)|\n",
      "Standard errors in parentheses."
     ]
    }
   ],
   "source": [
    "#%% Simulation & Estimation\n",
    "N = 100_000\n",
    "β_DGP = (0.15,0.25,-1.5)\n",
    "σ_DGP = 1.2\n",
    "y_OLS, X_OLS = DGP_OLS(N, β_DGP, σ_DGP)\n",
    "β_OLS, σ_OLS = OLS_estimation(y_OLS, X_OLS);\n",
    "β_OLS_GMM, σ_OLS_GMM = GMM_OLS(y_OLS,X_OLS);\n",
    "\n",
    "# Print the results\n",
    "print(\"Using the formula for OLS:\\n\")\n",
    "print(\"    |     β₀     |     β₁     |     β₂     | \\n\")\n",
    "@printf(\"DGP | %10.7f | %10.7f | %10.7f |\\n\", β_DGP[1], β_DGP[2], β_DGP[3])\n",
    "@printf(\"OLS | %10.7f | %10.7f | %10.7f |\\n\", β_OLS[1], β_OLS[2], β_OLS[3])\n",
    "@printf(\"    |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_OLS[1], σ_OLS[2], σ_OLS[3])\n",
    "@printf(\"GMM | %10.7f | %10.7f | %10.7f |\\n\", β_OLS_GMM[1], β_OLS_GMM[2], β_OLS_GMM[3])\n",
    "@printf(\"    |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_OLS_GMM[1], σ_OLS_GMM[2], σ_OLS_GMM[3])\n",
    "print(\"Standard errors in parentheses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, OLS and GMM estimation give us the same estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale up a little bit and do LIV. 2SLS in the over-identified case is what we will use in GMM.\n",
    "\n",
    "Let $f_i(\\beta):=f(Y_i,X_i,Z_i,\\beta)$ be the function of orthogonality conditions. The orthogonality condition for LIV is $\\mathbb{E}(\\varepsilon_i Z_i) = 0$, where $\\varepsilon_i = Y_i - X_i'\\beta$ and the expectation is taken over $i$. In the GMM framework, we wish to find $\\beta$ that satisfies below:\n",
    "$$\n",
    "    0 = \\frac{1}{N}\\sum_{i=1}^{N}f_i(\\beta) = \\frac{1}{N}\\sum_{i=1}^{N} \\varepsilon_i Z_i= \\frac{1}{N}\\sum_{i=1}^{N} (Y_i - X_i'\\beta) \\times Z_i.\n",
    "$$\n",
    "\n",
    "If we have more instruments than the number of endogenous variables, we call the model is over-identified. In the example below, I set $x_2$ to be the endogenous variable and provide 2 instruments, $z_1$ and $z_2$. If we use both $z_1$ and $z_2$, then we have 4 orthogonality conditions and 3 parameters to estimate. It is almost surely impossible to find $\\beta$ where the moment is zero. Thus, we just find $\\beta$ that makes the moment as close to zero as possible. We instead can introduce some positive definite weighting matrix $W$ and make the estimate more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP for LIV\n",
    "function DGP_LIV(N, β_DGP,μ,Σ)\n",
    "     Random.seed!(42)\n",
    "     (β₀,β₁,β₂) = β_DGP\n",
    "     (x₂, z₁, z₂, ε) = collect.(eachrow(rand(MvNormal(μ,Σ),N)))\n",
    "     x₁ = randn(N)\n",
    "     y = β₀ .+ β₁*x₁ .+ β₂*x₂ .+ ε # a Nx1 vector\n",
    "     X = [x₁ x₂] # a Nx2 matrix\n",
    "     Z = [z₁ z₂]\n",
    "     return y, X, Z\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function LIV_estimation(y, X_data, Z)\n",
    "\n",
    "     N = length(y)\n",
    "     X      = [ones(N) X_data]\n",
    "     X₁Z₁   = [ones(N) X[:,2] Z[:,1]]\n",
    "     X₁Z₂   = [ones(N) X[:,2] Z[:,2]]\n",
    "     X₁Z_oi = [ones(N) X[:,2] Z[:,1] Z[:,2]] # over-identified case\n",
    " \n",
    "     # Naive OLS produces biased estimates\n",
    "     β_IV_OLS  = inv(X'*X) * X'*y # a 3x1 vector\n",
    "     ε_OLS = y - X * β_IV_OLS\n",
    "     invXX = inv(X'*X)\n",
    "     Xε = X.*ε_OLS\n",
    "     Σ_IV_OLS = invXX*(Xε'*Xε)*invXX # V_HC0 - White-Heteroskedasticity-Robust VCov\n",
    "     σ_IV_OLS = sqrt.(diag(Σ_IV_OLS))\n",
    "\n",
    "     # Using Z₁ as instrument\n",
    "     # β_IV_Z₁ = inv(X'*X₁Z₁)*(X₁Z₁'*y)\n",
    "     # ε_IV_Z₁ = y - X * β_IV_Z₁\n",
    "     # invXZ₁  = inv(X'*X₁Z₁)\n",
    "     # Z₁ε     = X₁Z₁.*ε_IV_Z₁\n",
    "     # Σ_IV_Z₁ = invXZ₁*(Z₁ε'*Z₁ε)*invXZ₁\n",
    "     # σ_IV_Z₁ = sqrt.(diag(Σ_IV_Z₁))\n",
    "\n",
    "     # Using Z₂ as instrument\n",
    "     # β_IV_Z₂ = inv(X'*X₁Z₂)*(X₁Z₂'*y)\n",
    "     # ε_IV_Z₂ = y - X * β_IV_Z₂\n",
    "     # invXZ₂  = inv(X'*X₁Z₂)\n",
    "     # Z₂ε     = X₁Z₂.*ε_IV_Z₂\n",
    "     # Σ_IV_Z₂ = invXZ₂*(Z₂ε'*Z₂ε)*invXZ₂\n",
    "     # σ_IV_Z₂ = sqrt.(diag(Σ_IV_Z₂))\n",
    "\n",
    "     # 2SLS - Using Z₁ as instrument\n",
    "     β_2SLS_Z₁  = inv(X'*X₁Z₁*inv(X₁Z₁'*X₁Z₁)*X₁Z₁'*X)*X'*X₁Z₁*inv(X₁Z₁'*X₁Z₁)*X₁Z₁'*y\n",
    "     ε_2SLS_Z₁  = y - X * β_2SLS_Z₁\n",
    "     Zε_2SLS_Z₁ = X₁Z₁.*ε_2SLS_Z₁\n",
    "     Qxz₁  = X'*X₁Z₁\n",
    "     Qz₁z₁ = X₁Z₁'*X₁Z₁\n",
    "     Qz₁x  = X₁Z₁'*X\n",
    "     Ω_2SLS_Z₁ = Zε_2SLS_Z₁'*Zε_2SLS_Z₁\n",
    "     Σ_2SLS_Z₁ = inv(Qxz₁*inv(Qz₁z₁)*Qz₁x)*(Qxz₁*inv(Qz₁z₁)*Ω_2SLS_Z₁*inv(Qz₁z₁)*Qz₁x)*inv(Qxz₁*inv(Qz₁z₁)*Qz₁x)\n",
    "     σ_2SLS_Z₁ =sqrt.(diag(Σ_2SLS_Z₁))\n",
    "\n",
    "     # 2SLS - Using Z₂ as instrument\n",
    "     # β_2SLS_Z₂  = inv(X'*X₁Z₁*inv(X₁Z₁'*X₁Z₁)*X₁Z₁'*X)*X'*X₁Z₁*inv(X₁Z₁'*X₁Z₁)*X₁Z₁'*y\n",
    "     # ε_2SLS_Z₂  = y - X * β_2SLS_Z₂\n",
    "     # Zε_2SLS_Z₂ = X₁Z₂.*ε_2SLS_Z₂\n",
    "     # Qxz₂  = X'*X₁Z₂\n",
    "     # Qz₂z₂ = X₁Z₂'*X₁Z₂\n",
    "     # Qz₂x  = X₁Z₂'*X\n",
    "     # Ω_2SLS_Z₂ = Zε_2SLS_Z₂'*Zε_2SLS_Z₂\n",
    "     # Σ_2SLS_Z₂ = inv(Qxz₂*inv(Qz₂z₂)*Qz₂x)*(Qxz₂*inv(Qz₂z₂)*Ω_2SLS_Z₂*inv(Qz₂z₂)*Qz₂x)*inv(Qxz₂*inv(Qz₂z₂)*Qz₂x)\n",
    "     # σ_2SLS_Z₂ =sqrt.(diag(Σ_2SLS_Z₂))\n",
    "\n",
    "     # Using Z₁ and Z₂ as instrument: OVERIDENTIFIED!\n",
    "     Qxz = X'*X₁Z_oi\n",
    "     Qzz = X₁Z_oi'*X₁Z_oi\n",
    "     Qzx = X₁Z_oi'*X\n",
    "     β_2SLS_oi = inv(Qxz*inv(Qzz)*Qzx)*(Qxz*inv(Qzz)*X₁Z_oi')*y\n",
    "     ε_2SLS_oi = y - X*β_2SLS_oi\n",
    "     Zε = X₁Z_oi.*ε_2SLS_oi\n",
    "     Ω_2SLS_oi = Zε'*Zε\n",
    "     Σ_2SLS_oi= inv(Qxz*inv(Qzz)*Qzx)*(Qxz*inv(Qzz)*Ω_2SLS_oi*inv(Qzz)*Qzx)*inv(Qxz*inv(Qzz)*Qzx)\n",
    "     σ_2SLS_oi=sqrt.(diag(Σ_2SLS_oi))\n",
    "\n",
    "     # β_IV_Z₁,   σ_IV_Z₁,\n",
    "     # β_IV_Z₂,   σ_IV_Z₂,\n",
    "     # β_2SLS_Z₁, σ_2SLS_Z₁,\n",
    "     # β_2SLS_Z₂, σ_2SLS_Z₂,\n",
    "\n",
    "     return β_IV_OLS, σ_IV_OLS, β_2SLS_Z₁, σ_2SLS_Z₁, β_2SLS_oi, σ_2SLS_oi\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIV result: β₂ is biased in OLS \n",
      "        |     β₀     |     β₁     |     β₂     | \n",
      "DGP     |  0.1500000 |  0.2500000 | -1.5000000 |\n",
      "OLS     |  0.1479366 |  0.2514084 | -1.0057541 |\n",
      "        |( 0.0038011)|( 0.0037832)|( 0.0038027)|\n",
      "2SLS-Z₁ |  0.1467703 |  0.2517546 | -1.4979841 |\n",
      "        |( 0.0044668)|( 0.0044696)|( 0.0088246)|\n",
      "2SLS-oi |  0.1467379 |  0.2517642 | -1.5116826 |\n",
      "        |( 0.0044974)|( 0.0045002)|( 0.0044776)|\n",
      "Standard errors in parentheses."
     ]
    }
   ],
   "source": [
    "# order: x₂, Z₁, Z₂, ε\n",
    "μ = [0.0; 0.0; 0.0; 0.0];\n",
    "Σ = [2.0  0.5  -.5  1.0;\n",
    "     0.5  1.0  0.5  0.0;\n",
    "     -.5  0.5  1.0  0.0;\n",
    "     1.0  0.0  0.0  2.0];\n",
    "Y_LIV, X_LIV, Z_LIV = DGP_LIV(N, β_DGP,μ,Σ);\n",
    "β_IV_OLS, σ_IV_OLS, β_2SLS_Z₁, σ_2SLS_Z₁, β_2SLS_oi, σ_2SLS_oi = LIV_estimation(Y_LIV, X_LIV, Z_LIV);\n",
    "\n",
    "print(\"LIV result: β₂ is biased in OLS \\n\")\n",
    "print(\"        |     β₀     |     β₁     |     β₂     | \\n\")\n",
    "@printf(\"DGP     | %10.7f | %10.7f | %10.7f |\\n\", β_DGP[1], β_DGP[2], β_DGP[3])\n",
    "@printf(\"OLS     | %10.7f | %10.7f | %10.7f |\\n\", β_IV_OLS[1], β_IV_OLS[2], β_IV_OLS[3])\n",
    "@printf(\"        |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_OLS[1], σ_OLS[2], σ_OLS[3])\n",
    "#@printf(\"IV-Z₁   | %10.7f | %10.7f | %10.7f |\\n\", β_IV_Z₁[1], β_IV_Z₁[2], β_IV_Z₁[3])\n",
    "#@printf(\"        |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_IV_Z₁[1], σ_IV_Z₁[2], σ_IV_Z₁[3])\n",
    "#@printf(\"IV-Z₂   | %10.7f | %10.7f | %10.7f |\\n\", β_IV_Z₂[1], β_IV_Z₂[2], β_IV_Z₂[3])\n",
    "#@printf(\"        |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_IV_Z₂[1], σ_IV_Z₂[2], σ_IV_Z₂[3])\n",
    "@printf(\"2SLS-Z₁ | %10.7f | %10.7f | %10.7f |\\n\", β_2SLS_Z₁[1], β_2SLS_Z₁[2], β_2SLS_Z₁[3])\n",
    "@printf(\"        |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_2SLS_Z₁[1], σ_2SLS_Z₁[2], σ_2SLS_Z₁[3])\n",
    "#@printf(\"2SLS-Z₂ | %10.7f | %10.7f | %10.7f |\\n\", β_2SLS_Z₂[1], β_2SLS_Z₂[2], β_2SLS_Z₂[3])\n",
    "#@printf(\"        |(%7.4f)|(%7.4f)|(%7.4f)|\\n\", σ_2SLS_Z₂[1], σ_2SLS_Z₂[2], σ_2SLS_Z₂[3])\n",
    "@printf(\"2SLS-oi | %10.7f | %10.7f | %10.7f |\\n\", β_2SLS_oi[1], β_2SLS_oi[2], β_2SLS_oi[3])\n",
    "@printf(\"        |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_2SLS_oi[1], σ_2SLS_oi[2], σ_2SLS_oi[3])\n",
    "print(\"Standard errors in parentheses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that LIV estimate for $\\beta_2$ in over-identified case has smaller standard error compared to just-identified case. Motivated by these examples, let's take a look at what GMM does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition ([Bruce Hansen, p.416](https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf))\n",
    "Let $\\beta$ be a $k\\times1$ vector, $f_i(\\beta)$ be a $l\\times1$ orthogonality conditions, and $W$ be a $l\\times l$ positive definite weighting matrix. Define the GMM criterion function $J(\\beta)$ as\n",
    "$$\n",
    "    J(\\beta) = N \\left(\\frac{1}{N}\\sum_{i=1}^N f_i(\\beta)\\right)' W \\left(\\frac{1}{N}\\sum_{i=1}^N f_i(\\beta)\\right).\n",
    "$$\n",
    "The GMM estimator is \n",
    "$$\n",
    "    \\hat{\\beta}_{gmm} = \\underset{\\beta}{\\arg\\min}\\; J(\\beta).\n",
    "$$\n",
    "\n",
    "#### Proposition ([Bruce Hansen, p.432](https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf))\n",
    "Under general regularity conditions, $\\sqrt{N}(\\hat{\\beta}_{gmm}-\\beta_0) \\xrightarrow[]{d} N(0,V_\\beta)$ where \n",
    "$$\n",
    "\\begin{align*}\n",
    "    V_\\beta &= (Q'WQ)^{-1}(Q'W\\Omega WQ)(Q'WQ)^{-1}\\\\\n",
    "    \\Omega &= \\mathbb{E}(f_i f_i') \\\\\n",
    "    Q &= \\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta'}f_i(\\beta)\\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "If $W$ is efficient, $V_\\beta = (Q'\\Omega^{-1} Q)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you compute standard errors for $\\hat{\\beta}_{gmm}$, you must divide $V_\\beta$ by $N$! $V_\\beta$ is the variance for $\\sqrt{N}(\\hat{\\beta}_{gmm}-\\beta)$, not $\\hat{\\beta}_{gmm}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two-step GMM ([Bruce Hansen, p.444](https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf))\n",
    "In order to construct the efficient GMM estimator, we need to know what is the optimal $W$ in our model. In LIV, we know that $W=(Z'Z)^{-1}$. If the optimal weighting matrix $W=\\Omega^{-1}$ is unknown, then we can first estimate $W$ with a standard GMM routine (say, by setting $W=I$), and then do the GMM routine again with $W=\\hat{\\Omega}^{-1}$.\n",
    "\n",
    "#### Programming\n",
    "In this case, we can simply code GMM as follows:\n",
    "1. Set some weighting matrix $W$. In our example, for the just-identified case, use $W=I$ and for the over-identified case, use $W=(Z'Z)^{-1}$.\n",
    "1. Construct the GMM criterion function. It is **obj_IV** in my code.\n",
    "1. Ask the computer to minimize the criterion function for you. The minimizer is your $\\hat{\\beta}_{gmm}$.\n",
    "1. Compute $Q$ and $\\Omega$.\n",
    "    - If you are using a two-step GMM, then repeat step 3 and 4 by setting $W=\\hat{\\Omega}^{-1}$\n",
    "1. Compute the asymptotic variance $V_\\beta$. \n",
    "1. Compute standard errors with $\\textbf{sqrt}.(\\textbf{diag}(\\mathbf{V_\\beta}/N))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GMM_IV\n",
    "function GMM_one_step(y,Z_data,X_data;W=I)\n",
    "\n",
    "    #=\n",
    "    f_IV(β, y, Z, X): orthogonality conditions\n",
    "    W: the weighting matrix. In one-step GMM, we do not update this.\n",
    "    =#\n",
    "   \n",
    "    N = length(y)\n",
    "    X = [ones(N) X_data]\n",
    "    Z = [ones(N) X_data[:,1] Z_data]\n",
    "    k = size(X)[2] # size of β\n",
    "    \n",
    "    function f_IV(β, y, Z, X)\n",
    "        return (y - X*β) .* Z # (N×1) * (N×4)\n",
    "    end\n",
    "    \n",
    "    f_sum_IV(β) = vec(sum(f_IV(β, y, Z, X),dims=1));\n",
    "    obj_IV(β) = N * (f_sum_IV(β)/N)' * W * (f_sum_IV(β)/N);\n",
    "    opt = optimize(obj_IV, randn(k), BFGS(), autodiff = :forward);\n",
    "    β_GMM = opt.minimizer\n",
    "    Q = ForwardDiff.jacobian(β -> f_sum_IV(β) , β_GMM)/N;\n",
    "    Ω = f_IV(β_GMM, y, Z, X)'*f_IV(β_GMM, y, Z, X)/N;\n",
    "    V_GMM = inv(Q'*W*Q)*(Q'*W*Ω*W*Q)*inv(Q'*W*Q)/N; # remember to divide by N!\n",
    "    σ_GMM = sqrt.(diag(V_GMM))\n",
    "\n",
    "    return β_GMM, σ_GMM\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GMM_two_step(y,Z_data,X_data)\n",
    "\n",
    "    #=\n",
    "    f_IV(β, y, Z, X): orthogonality conditions\n",
    "    W: the weighting matrix. In this case, we update this once.\n",
    "    =#\n",
    "\n",
    "    N = length(y)\n",
    "    X = [ones(N) X_data]\n",
    "    Z = [ones(N) X_data[:,1] Z_data]\n",
    "    k = size(X)[2] # size of β\n",
    "    \n",
    "    function f_IV(β, y, Z, X)\n",
    "        return (y - X*β) .* Z # (N×1) * (N×4)\n",
    "    end\n",
    "    f_sum_IV(β) = vec(sum(f_IV(β, y, Z, X),dims=1));\n",
    "    \n",
    "    # First-step: the estimates and the standard errors are the same as setting W=I in one-step GMM.\n",
    "    W_1 = I;\n",
    "    obj_GMM_1(β) = N * (f_sum_IV(β)/N)' * W_1 * (f_sum_IV(β)/N);\n",
    "    opt_1 = optimize(obj_GMM_1, randn(k), BFGS(), autodiff = :forward);\n",
    "    β_GMM_1 = opt_1.minimizer\n",
    "    Q_1 = ForwardDiff.jacobian(β -> f_sum_IV(β) , β_GMM_1)/N;\n",
    "    Ω_1 = f_IV(β_GMM_1, y, Z, X)'*f_IV(β_GMM_1, y, Z, X)/N;\n",
    "\n",
    "    # Second-step\n",
    "    W_opt = inv(Ω_1)\n",
    "    obj_GMM_2(β) = N * (f_sum_IV(β)/N)' * W_opt * (f_sum_IV(β)/N);\n",
    "    opt_2 = optimize(obj_GMM_2, randn(k), BFGS(), autodiff = :forward);\n",
    "    β_GMM_2 = opt_2.minimizer\n",
    "    Q_2 = ForwardDiff.jacobian(β -> f_sum_IV(β) , β_GMM_2)/N;\n",
    "    Ω_2 = f_IV(β_GMM_2, y, Z, X)'*f_IV(β_GMM_2, y, Z, X)/N;\n",
    "    \n",
    "    # Results\n",
    "    V_GMM_1 = inv(Q_1'*W_1*Q_1)*(Q_1'*W_1*Ω_1*W_1*Q_1)*inv(Q_1'*W_1*Q_1)/N; # remember to divide by N!\n",
    "    σ_GMM_1 = sqrt.(diag(V_GMM_1))\n",
    "    V_GMM_2 = inv(Q_2'*W_opt*Q_2)*(Q_2'*W_opt*Ω_2*W_opt*Q_2)*inv(Q_2'*W_opt*Q_2)/N; # remember to divide by N!\n",
    "    σ_GMM_2 = sqrt.(diag(V_GMM_2))\n",
    "\n",
    "    return β_GMM_1, σ_GMM_1, β_GMM_2, σ_GMM_2\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM result, overidentified: β₂ is biased in OLS \n",
      "              |     β₀    |     β₁    |     β₂    | \n",
      "DGP           |  0.1500000 |  0.2500000 | -1.5000000 |\n",
      "One-step GMM: |  0.1467573 |  0.2517024 | -1.5115486 |\n",
      " W=I          |( 0.0044970)|( 0.0045002)|( 0.0044782)|\n",
      "One-step GMM: |  0.1467379 |  0.2517642 | -1.5116826 |\n",
      " W=inv(Z'Z)   |( 0.0044974)|( 0.0045002)|( 0.0044776)|\n",
      "Two-step GMM  |  0.1467965 |  0.2517965 | -1.5117060 |\n",
      "              |( 0.0044973)|( 0.0045002)|( 0.0044777)|\n",
      "Standard errors in parentheses."
     ]
    }
   ],
   "source": [
    "# Estimation\n",
    "Z = [ones(N) X_LIV[:,1] Z_LIV];\n",
    "β_GMM_I,  σ_GMM_I      = GMM_one_step(Y_LIV,Z_LIV,X_LIV,W=I);\n",
    "β_GMM_ZZ, σ_GMM_ZZ     = GMM_one_step(Y_LIV,Z_LIV,X_LIV,W=inv(Z'*Z));\n",
    "_, _, β_GMM_2, σ_GMM_2 = GMM_two_step(Y_LIV,Z_LIV,X_LIV);\n",
    "\n",
    "# Print the results\n",
    "print(\"GMM result, overidentified: β₂ is biased in OLS \\n\")\n",
    "print(\"              |     β₀    |     β₁    |     β₂    | \\n\")\n",
    "@printf(\"DGP           | %10.7f | %10.7f | %10.7f |\\n\", β_DGP[1], β_DGP[2], β_DGP[3])\n",
    "@printf(\"One-step GMM: | %10.7f | %10.7f | %10.7f |\\n\", β_GMM_I[1], β_GMM_I[2], β_GMM_I[3])\n",
    "@printf(\" W=I          |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_GMM_I[1], σ_GMM_I[2], σ_GMM_I[3])\n",
    "@printf(\"One-step GMM: | %10.7f | %10.7f | %10.7f |\\n\", β_GMM_ZZ[1], β_GMM_ZZ[2], β_GMM_ZZ[3])\n",
    "@printf(\" W=inv(Z'Z)   |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_GMM_ZZ[1], σ_GMM_ZZ[2], σ_GMM_ZZ[3])\n",
    "@printf(\"Two-step GMM  | %10.7f | %10.7f | %10.7f |\\n\", β_GMM_2[1], β_GMM_2[2], β_GMM_2[3])\n",
    "@printf(\"              |(%10.7f)|(%10.7f)|(%10.7f)|\\n\", σ_GMM_2[1], σ_GMM_2[2], σ_GMM_2[3])\n",
    "print(\"Standard errors in parentheses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Professor Miller's Lecture Notes](https://comlabgames.com/structuraleconometrics/)\n",
    "- [Bruce Hansen's econometrics textbook](https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf)\n",
    "- Hansen, Lars Peter, and Kenneth J. Singleton. \"Generalized instrumental variables estimation of nonlinear rational expectations models.\" Econometrica: Journal of the Econometric Society (1982): 1269-1286.\n",
    "- Hansen, Lars Peter. \"Large sample properties of generalized method of moments estimators.\" Econometrica: Journal of the econometric society (1982): 1029-1054.\n",
    "- Newey, Whitney K., and Daniel McFadden. \"Large sample estimation and hypothesis testing.\" Handbook of econometrics 4 (1994): 2111-2245."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "863fb66d7857448b058055df85df06b40a6efc78dc2e94b3b9fd236ddb7af4ee"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
